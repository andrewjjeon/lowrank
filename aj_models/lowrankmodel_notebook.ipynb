{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi'] = 200\n",
    "import matplotlib.colors as colors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import numpy.linalg as la\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "\n",
    "    # X is basically torch.tensor(y_session), U is basically torch.tensor(u_session)\n",
    "    def __init__(self, X, U, num_lags):\n",
    "        self.X = X\n",
    "        self.U = U \n",
    "        self.num_lags = num_lags\n",
    "\n",
    "    def __len__(self):\n",
    "        # The dataset length is reduced by num_lags due to the dependency on previous data points\n",
    "        return len(self.X) - self.num_lags\n",
    "\n",
    "# NOTE: This function is returning 4 index history of X and U as well as the prediction point X\n",
    "    def __getitem__(self, index):\n",
    "        # return slices of X and U of [index:index + num_lags]\n",
    "        X_history = [self.X[index + i] for i in range(self.num_lags)]\n",
    "        U_history = [self.U[index + i] for i in range(self.num_lags)]\n",
    "\n",
    "        # X_next is the prediction point\n",
    "        X_next = self.X[index + self.num_lags]\n",
    "        return X_history, U_history, X_next\n",
    "\n",
    "# NOTE: This function normalizes matrix such that the largest singular value becomes 2\n",
    "def singular_value_norm(matrix):\n",
    "    norm_val = torch.linalg.norm(matrix, 2)  # norm_val = largest singular value of matrix\n",
    "    if norm_val > 2: \n",
    "        matrix = 2 * matrix / norm_val  # normalized/scaled such that the largest singular values becomes 2\n",
    "    return matrix\n",
    "\n",
    "\n",
    "class LinearDynamicModel(nn.Module):  # inherit from nn.Module which is base class for all neural network in pytorch\n",
    "    def __init__(self, state_dim, input_dim, num_lags, init_value = None):\n",
    "        \n",
    "        super(LinearDynamicModel, self).__init__()  # constructor of nn.Module, gives functionalities of nn.Module to LinearDynamicModel\n",
    "        \n",
    "        # if doesn't exist yet, create a new parameter list\n",
    "        if init_value is None:\n",
    "            # Create diagonal matrices for alpha and beta, one for each lag\n",
    "\n",
    "            # nn.ParameterList is like Python list, but tensors that are nn.Parameter are visible by all Module methods and autograd will work\n",
    "            # torch.randn(state_dim) creates tensor of len(state_dim) of random numbers from normal distribution (0, 1)\n",
    "            # alpha and beta are lists of parameters with num_lags=4 tensors of size state_dim=502\n",
    "            self.alpha = nn.ParameterList([nn.Parameter(torch.randn(state_dim)) for _ in range(num_lags)])\n",
    "            self.beta = nn.ParameterList([nn.Parameter(torch.randn(state_dim)) for _ in range(num_lags)])\n",
    "            \n",
    "            # W is matrix A and B is matrix B in the paper\n",
    "            # W and B are lists of parameters with num_lags=4 tensors of size (state_dim, state_dim) and (state_dim, input_dim)\n",
    "            self.W = nn.ParameterList([nn.Parameter(torch.randn(state_dim, state_dim)) for _ in range(num_lags)])\n",
    "            self.B = nn.ParameterList([nn.Parameter(torch.randn(state_dim, input_dim)) for _ in range(num_lags)]) # this is full-rank, linear model so i think state_dim=input_dim\n",
    "\n",
    "            # V is a parameter tensor of size state_dim\n",
    "            self.V = nn.Parameter(torch.randn(state_dim))\n",
    "\n",
    "        # if already exists, create the parameter list by pulling from the existing dictionary init_value\n",
    "        else:\n",
    "            # init_value is a {} dictionary of [] lists\n",
    "            self.alpha = nn.ParameterList([nn.Parameter(init_value['alpha'][i]) for i in range(num_lags)])\n",
    "            self.beta = nn.ParameterList([nn.Parameter(init_value['beta'][i]) for i in range(num_lags)])\n",
    "\n",
    "            self.W = nn.ParameterList([nn.Parameter(init_value['W'][i]) for i in range(num_lags)])\n",
    "            self.B = nn.ParameterList([nn.Parameter(init_value['B'][i]) for _ in range(num_lags)])\n",
    "\n",
    "            self.V = nn.Parameter(init_value['V'])\n",
    "    \n",
    "\n",
    "    def forward(self, X_history, U_history):\n",
    "        # initialize tensor of 0s of same size as first X_history tensor so we would expect X_next to be size (502)\n",
    "        X_next = torch.zeros_like(X_history[0])\n",
    "\n",
    "        #  self.W is ParameterList of 4 tensors of size (502, 502)\n",
    "        #  self.alpha is ParameterList of 4 tensors of size (502)\n",
    "        #  X_history is python list of size (4, 502)\n",
    "        for W_k, alpha_k, X_k in zip(self.W, self.alpha, X_history):\n",
    "            # W_k (502, 502)\n",
    "            # alpha_k (502)\n",
    "            # X_k (502)\n",
    "\n",
    "            X_k = X_k.unsqueeze(-1)\n",
    "            alpha_diag_k = torch.diag(alpha_k)\n",
    "\n",
    "            # unsqueeze X_k so it now has shape (502, 1), add extra dimension\n",
    "            # torch.diag(alpha_k) so alpha_diag_k has shape (502, 502)\n",
    "\n",
    "            # compute contribution of state X_k to state X_next\n",
    "            # matrices A and B correspond to W + diag(alpha) and B + diag(beta)\n",
    "            # (502, 502) @ (502, 1) = (502,1).squeeze(-1) = (502)\n",
    "            contribution = torch.matmul(singular_value_norm(W_k + alpha_diag_k), X_k).squeeze(-1)\n",
    "\n",
    "            # X_next is (502) of 0's so add contribution to it\n",
    "            # X_next now has the contribution of num_lags previous states\n",
    "            X_next += contribution\n",
    "\n",
    "        #  self.B is ParameterList of 4 tensors of size (502, input_dim)\n",
    "        #  self.beta is ParameterList of 4 tensors of size (502)\n",
    "        #  U_history is python list of size (4, 502)\n",
    "        for B_k, beta_k, U_k in zip(self.B, self.beta, U_history):\n",
    "            U_k = U_k.unsqueeze(-1)\n",
    "            beta_diag_k = torch.diag(beta_k)\n",
    "            # B_k (502, input_dim=502), full rank model so state_dim = input_dim\n",
    "            # U_k (502, 1)\n",
    "            # beta_diag_k (502, 502)\n",
    "\n",
    "            # compute contribution of input U_k to next state X-next\n",
    "            # (502,502) @ (502,1) = (502,1).squeeze(-1) = (502)\n",
    "            contribution = torch.matmul(singular_value_norm(B_k + beta_diag_k), U_k).squeeze(-1)\n",
    "\n",
    "            # X_next now has contribution of num_lags previous states AND num_lags previous inputs\n",
    "            # X_next is still (502)\n",
    "            X_next += contribution\n",
    "\n",
    "        # X_next is still (502)\n",
    "        X_next += self.V[None, :]\n",
    "        return X_next\n",
    "    \n",
    "class LowRankLinearDynamicModel(nn.Module):  # inherit from nn.Module which is base class for all neural network in pytorch\n",
    "    def __init__(self, state_dim, input_dim, rank_dim, num_lags, init_value = None):\n",
    "        super(LowRankLinearDynamicModel, self).__init__()  # constructor of nn.Module, gives functionalities of nn.Module to LinearDynamicModel\n",
    "        \n",
    "        if init_value is None:\n",
    "            # self.alpha and self.beta are both ParameterList of 4 Parameters each a tensor of size (502)\n",
    "            self.alpha = nn.ParameterList([nn.Parameter(torch.randn(state_dim)) for _ in range(num_lags)])\n",
    "            self.beta = nn.ParameterList([nn.Parameter(torch.randn(state_dim)) for _ in range(num_lags)])\n",
    "\n",
    "            # self.W_u and self.W_v are both ParameterList of 4 Parameters each a tensor of size (502, 5)\n",
    "            self.W_u = nn.ParameterList([nn.Parameter(torch.randn(state_dim, rank_dim)) for _ in range(num_lags)])\n",
    "            self.W_v = nn.ParameterList([nn.Parameter(torch.randn(state_dim, rank_dim)) for _ in range(num_lags)])\n",
    "\n",
    "            # self.B_u and self.B_v are both ParameterList of 4 Parameters each a tensor of size (502, 5)\n",
    "            self.B_u = nn.ParameterList([nn.Parameter(torch.randn(state_dim, rank_dim)) for _ in range(num_lags)])\n",
    "            self.B_v = nn.ParameterList([nn.Parameter(torch.randn(state_dim, rank_dim)) for _ in range(num_lags)])\n",
    "\n",
    "            # self.V is a Parameter tensor of size (502)\n",
    "            self.V = nn.Parameter(torch.randn(state_dim))\n",
    "        else:\n",
    "            self.alpha = nn.ParameterList([nn.Parameter(init_value['alpha'][i]) for i in range(num_lags)])\n",
    "            self.beta = nn.ParameterList([nn.Parameter(init_value['beta'][i]) for i in range(num_lags)])\n",
    "\n",
    "            self.W_u = nn.ParameterList([nn.Parameter(init_value['W_u'][i]) for i in range(num_lags)])\n",
    "            self.W_v = nn.ParameterList([nn.Parameter(init_value['W_v'][i]) for i in range(num_lags)])\n",
    "\n",
    "            self.B_u = nn.ParameterList([nn.Parameter(init_value['B_u'][i]) for _ in range(num_lags)])\n",
    "            self.B_v = nn.ParameterList([nn.Parameter(init_value['B_v'][i]) for _ in range(num_lags)])\n",
    "\n",
    "            self.V = nn.Parameter(init_value['V'])\n",
    "        \n",
    "    def forward(self, X_history, U_history):\n",
    "        X_next = torch.zeros_like(X_history[0])  # (502)\n",
    "        for W_u_k, W_v_k, alpha_k, X_k in zip(self.W_u, self.W_v, self.alpha, X_history):\n",
    "            X_k = X_k.unsqueeze(-1)  \n",
    "            alpha_diag_k = torch.diag(alpha_k)\n",
    "            # W_u_k and W_v_k(502, 35) low rank approx. of matrix A\n",
    "            # alpha_diag_k (502, 502) original diagonals of each of the 4 (502,502) in Ahat ~ y_session[t:t+4]\n",
    "            # X_k (502, 1) each of the 4 previous states\n",
    "\n",
    "            # U_A @ V_A.T (502, 502)\n",
    "            W_k = torch.mm(W_u_k, W_v_k.T)  # reconstruct each of 4 A matrix \n",
    "            \n",
    "            # A_s = U_A @ V_A.T + D_A\n",
    "            # (502, 502) @ (502, 1) = (502, 1).squeeze(-1) = (502)\n",
    "            contribution = torch.matmul(singular_value_norm(W_k + alpha_diag_k), X_k).squeeze(-1)  # Shape returns to (batch_size, state_dim)\n",
    "            X_next += contribution  # add each of the 4 contributions from the previous states to x_next (502,)\n",
    "\n",
    "        for B_u_k, B_v_k, beta_k, U_k in zip(self.B_u, self.B_v, self.beta, U_history):\n",
    "            U_k = U_k.unsqueeze(-1)\n",
    "            beta_diag_k = torch.diag(beta_k)\n",
    "            # B_u_k and B_v_k (502, 35) low rank approx. of matrix B\n",
    "            # beta_diag_k (502, 502) (502, 502) original diagonals of each of the 4 (502,502) in Ahat ~ u_session[t:t+4]\n",
    "            # U_k (502, 1) each of the 4 previous inputs\n",
    "\n",
    "            # U_B @ V_B.T (502, 502)\n",
    "            B_k = torch.mm(B_u_k, B_v_k.T)  # reconstruct each of 4 B matrix \n",
    "            \n",
    "            # B_s = U_B @ V_B.T + D_B\n",
    "            # (502, 502) @ (502, 1) = (502, 1).squeeze(-1) = (502)\n",
    "            contribution = torch.matmul(singular_value_norm(B_k + beta_diag_k), U_k).squeeze(-1)\n",
    "            X_next += contribution  # add each of the 4 contributions from the previous inputs to x_next (502,)\n",
    "\n",
    "        # X_next is (502,) of all contributions from each of the previous 4 states and inputs    \n",
    "        X_next += self.V[None, :]  # account for bias term\n",
    "        return X_next\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=100, lr=0.01, clip_value=1.0, l2_lambda=0.01, step_size=50, gamma=0.5, checkpoint_name = 'linear_35'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # Decays the lr of each parameter by a factor of gamma at every step_size\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Lists to track loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    l2_penalty_losses = []\n",
    "    \n",
    "    current_lr = lr\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()  # set model to training mode\n",
    "        total_train_loss = 0\n",
    "        total_l2_penaty = 0\n",
    "\n",
    "        for X_history, U_history, X_next in train_loader:  # loop through 12 train batches in train_loader\n",
    "            optimizer.zero_grad()  # zero out gradients for each batch\n",
    "            predictions = model(X_history, U_history)  # this is X_next (502) from forward method\n",
    "            loss = criterion(predictions, X_next)  # take loss betwen X_next (502) predicted and X_next true\n",
    "            # print(f\"pre_L2_loss: {loss}\")\n",
    "            \n",
    "            # total_train_loss += loss.item()\n",
    "            \n",
    "            # Compute the L2 penalty for each parameter\n",
    "            l2_penalty = torch.tensor(0.).to(device)\n",
    "            \n",
    "            for param in model.W_u:\n",
    "                l2_penalty += torch.norm(param,p=2)\n",
    "            # print(f\"l2_penalty is {l2_penalty}\")\n",
    "            for param in model.W_v:\n",
    "                l2_penalty += torch.norm(param,p=2)\n",
    "            for param in model.B_u:\n",
    "                l2_penalty += torch.norm(param,p=2)\n",
    "            for param in model.B_v:\n",
    "                l2_penalty += torch.norm(param,p=2)\n",
    "            for param in model.alpha:\n",
    "                l2_penalty += torch.norm(param,p=2)\n",
    "            for param in model.beta:\n",
    "                l2_penalty += torch.norm(param,p=2)\n",
    "            \n",
    "            # Add the L2 penalty to the original loss\n",
    "            loss += l2_lambda * l2_penalty\n",
    "            \n",
    "            total_train_loss += loss.item()  # ??? Maybe this is why its higher than val loss.\n",
    "\n",
    "            # print(f\"post_L2_loss: {loss}\")\n",
    "\n",
    "            total_l2_penaty += l2_penalty.item()\n",
    "\n",
    "            # accumulates dloss/dx for every parameter x into x.grad for every parameter x\n",
    "            # x.grad += dloss/dx\n",
    "            loss.backward()\n",
    "\n",
    "            # x += -lr * x.grad, update parameters with gradients\n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_history, U_history, X_next in val_loader:  # loop through 4 val batches of 2000 in val_loader\n",
    "                predictions = model(X_history, U_history)\n",
    "                loss = criterion(predictions, X_next)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        # Logging average training and validation loss, and L2 penalty\n",
    "        # TODO: need some clarity if this is computing the loss from 1 sample from the batch of 2000 or somehow doing it for all points, i think the gap is in how __getitem__ works\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "        val_loss = total_val_loss / len(val_loader)\n",
    "        l2_penaty_loss = total_l2_penaty / len(train_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        l2_penalty_losses.append(l2_penaty_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}: Train Loss = {train_loss}, Val Loss = {val_loss}, L2 Penalty Loss = {l2_penaty_loss}, LR = {current_lr}')\n",
    "\n",
    "        # Checkpointing based on minimal validation loss across all epochs\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "            checkpoint_path = f'checkpoints/model_best_' + checkpoint_name + '.pt'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint saved at epoch {epoch} with Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "    # Plotting the training and validation losses\n",
    "    plt.figure(figsize=(12, 6), dpi=80)\n",
    "    plt.plot(train_losses, label='Training Loss')  #if no x provided, plots against indices\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(f\"Training and Validation Loss w/ L2 Reg of lambda {l2_lambda}\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plotting the L2 penalties\n",
    "    plt.figure(figsize=(12, 6), dpi=80)\n",
    "    plt.plot(l2_penalty_losses, label='L2 Penalty', color='red')\n",
    "    plt.title(f\"L2 Penalty Over Epochs of lambda {l2_lambda}\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('L2 Penalty')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
